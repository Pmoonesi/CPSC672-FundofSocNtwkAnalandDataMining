{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7cb433d-893d-482f-bc6e-7e178d82f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4a1c73-417f-4a3e-b44d-c89112c7a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Processed/grouped_augmented_trips.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09384069-4bc8-4fbc-b2b8-1c42d435a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_members, df_casuals = df[df['User Type'] == 'Member'], df[df['User Type'] == 'Casual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3ef48f2-d39f-4a9b-aaba-c914d2fcf7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2370904/448852404.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_members['Start Time'] = pd.to_datetime(df_members['Start Time'])\n",
      "/tmp/ipykernel_2370904/448852404.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_members['End Time'] = pd.to_datetime(df_members['End Time'])\n"
     ]
    }
   ],
   "source": [
    "df_members['Start Time'] = pd.to_datetime(df_members['Start Time'])\n",
    "df_members['End Time'] = pd.to_datetime(df_members['End Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "605b9474-3ec3-4eb1-86d6-3abe01c6725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = min(df_members['Start Time'].min().floor('d'), df_members['End Time'].min().floor('d'))\n",
    "end_date = max(df_members['Start Time'].max().ceil('d'), df_members['End Time'].max().ceil('d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d03d8-26b4-4edb-ab99-6653db454d35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ca7b1a1-dd24-4d54-89a0-6429903ac234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from dateutil.relativedelta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34c82197-4557-4937-b9a5-53f39dc7d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = date(start_date.year, 1, 1)\n",
    "years_begin = [start_year + relativedelta(years=+i) for i in range(end_date.year - start_date.year + 1)]\n",
    "years_end = [d + relativedelta(years=+1) for d in years_begin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3265df83-536a-48fb-b1a9-10d80a81913d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660fc192bfd641938c05a4a0fd62a597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import date\n",
    "from dateutil.relativedelta import * \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "years_interval = list(zip(years_begin, years_end))\n",
    "\n",
    "for year in tqdm(years_interval):\n",
    "\n",
    "    year_begin, year_end = year\n",
    "    \n",
    "    year_df = df_members[(df_members['Start Time'] >= pd.to_datetime(year_begin)) & (df_members['End Time'] < pd.to_datetime(year_end))]\n",
    "\n",
    "    year_df.to_csv(f'Processed/casuals/{year_begin.year}/trips-{year_begin.year}.csv')\n",
    "\n",
    "    # G, _ = func2(year_df)\n",
    "    \n",
    "    # nx.write_gml(G, f\"Processed/years/month-{month}.gml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d471a-724e-490b-9246-3ea131fcc58e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0036fdc-603d-4652-8933-c6f91eb0c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_month = start_date.replace(day=1)\n",
    "\n",
    "while start_month < end_date:\n",
    "\n",
    "    end_month = start_month + relativedelta(months=+1)\n",
    "\n",
    "    month_df = df_members[(df_members['Start Time'] >= pd.to_datetime(start_month)) & (df_members['End Time'] < pd.to_datetime(end_month))]\n",
    "\n",
    "    month_df.to_csv(f'Processed/casuals/{start_month.year}/months/trips-{start_month.year}-{start_month.month}.csv')\n",
    "\n",
    "    start_month = end_month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140b3c5-a813-44ba-b0df-9eabdd8a9162",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2729ec3-2053-4479-9f24-107d090e5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_week = start_date.replace(day=1)\n",
    "\n",
    "# while start_week < end_date:\n",
    "\n",
    "#     end_week = start_week + relativedelta(weeks=+1)\n",
    "\n",
    "#     week_df = df_members[(df_members['Start Time'] >= pd.to_datetime(start_week)) & (df_members['End Time'] < pd.to_datetime(end_week))]\n",
    "\n",
    "#     year, week, _ = end_week.isocalendar()\n",
    "\n",
    "#     week_df.to_csv(f'Processed/casuals/{year}/weeks/trips-{year}-{week}.csv')\n",
    "\n",
    "#     start_week = end_week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e670e-a068-42f6-8df2-6f752821c862",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c8e8864-6a05-456d-97a6-a8a1946734a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_edges = np.linspace(df['temperature'].min(), df['temperature'].max() + 0.01, 7)\n",
    "temp_intervals = list(zip(temp_edges[:-1], temp_edges[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7098dbb-9fc7-499b-a532-4dea6f95a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = date(start_date.year, 1, 1)\n",
    "years_begin = [start_year + relativedelta(years=+i) for i in range(end_date.year - start_date.year + 1)]\n",
    "years_end = [d + relativedelta(years=+1) for d in years_begin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7943c16c-8dca-4594-8869-ae12d4701a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82f1d60be154db790f526fc303a6b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import date\n",
    "from dateutil.relativedelta import * \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "years_interval = list(zip(years_begin, years_end))\n",
    "\n",
    "for year in tqdm(years_interval):\n",
    "\n",
    "    year_begin, year_end = year\n",
    "    \n",
    "    year_df = df_members[(df_members['Start Time'] >= pd.to_datetime(year_begin)) & (df_members['End Time'] < pd.to_datetime(year_end))]\n",
    "\n",
    "    for i, t_interval in enumerate(temp_intervals):\n",
    "\n",
    "        year_temp_df = year_df[(year_df['temperature'] >= t_interval[0]) & (year_df['temperature'] < t_interval[1])]\n",
    "        \n",
    "        year_temp_df.to_csv(f'Processed/members/{year_begin.year}/temp/trips-temp-{i}.csv')\n",
    "\n",
    "    # G, _ = func2(year_df)\n",
    "    \n",
    "    # nx.write_gml(G, f\"Processed/years/month-{month}.gml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f8b21-1373-45f5-b3df-074d40057357",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## csv to gml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "444f085d-9dbc-4dba-a6dd-663cd817acec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc04115f634a8191c550ca8d42ae31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed/members/2024/temp/trips-temp-0.csv\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "temp_csvs = glob.glob('Processed/members/**/temp/*.csv', recursive=True)\n",
    "\n",
    "for temp_csv in tqdm(temp_csvs):\n",
    "\n",
    "    try:\n",
    "\n",
    "        new_name = temp_csv.replace('trips-temp', 'network-temp').replace('csv', 'gml')\n",
    "        temp_df = pd.read_csv(temp_csv)\n",
    "    \n",
    "        G, _ = func2(temp_df)\n",
    "        nx.write_gml(G, new_name)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(temp_csv)\n",
    "        print(len(temp_df))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fdad8c96-0aa5-43d2-a115-6c38c7d750cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789f4e00528447dfbf2f48871da2b346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "temp_csvs = glob.glob('Processed/casuals/**/temp/*.csv', recursive=True)\n",
    "\n",
    "for temp_csv in tqdm(temp_csvs):\n",
    "\n",
    "    try:\n",
    "\n",
    "        new_name = temp_csv.replace('trips-temp', 'network-temp').replace('csv', 'gml')\n",
    "        temp_df = pd.read_csv(temp_csv)\n",
    "    \n",
    "        G, _ = func2(temp_df)\n",
    "        nx.write_gml(G, new_name)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(temp_csv)\n",
    "        print(len(temp_df))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8bd9cb3-c473-4036-a20b-4662d78c4982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76860c769bdc41829edbee87cc4e21f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed/members/2023/months/trips-2023-9.csv\n",
      "0\n",
      "Processed/members/2023/months/trips-2023-10.csv\n",
      "0\n",
      "Processed/members/2023/months/trips-2023-11.csv\n",
      "0\n",
      "Processed/members/2023/months/trips-2023-12.csv\n",
      "0\n",
      "Processed/members/2024/months/trips-2024-1.csv\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "month_csvs = glob.glob('Processed/members/**/months/*.csv', recursive=True)\n",
    "\n",
    "for month_csv in tqdm(month_csvs):\n",
    "\n",
    "    try:\n",
    "\n",
    "        new_name = month_csv.replace('trips-', 'network-').replace('csv', 'gml')\n",
    "        month_df = pd.read_csv(month_csv)\n",
    "    \n",
    "        G, _ = func2(month_df)\n",
    "        nx.write_gml(G, new_name)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(month_csv)\n",
    "        print(len(month_df))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ecd27115-6083-48cd-b4de-e045b0af3512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d60f188ac44d54aca61143e2a0562d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "month_csvs = glob.glob('Processed/casuals/**/months/*.csv', recursive=True)\n",
    "\n",
    "for month_csv in tqdm(month_csvs):\n",
    "\n",
    "    try:\n",
    "\n",
    "        new_name = month_csv.replace('trips-', 'network-').replace('csv', 'gml')\n",
    "        month_df = pd.read_csv(month_csv)\n",
    "    \n",
    "        G, _ = func2(month_df)\n",
    "        nx.write_gml(G, new_name)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(month_csv)\n",
    "        print(len(month_df))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fdf5d-83f1-4bac-be94-59d3615ab161",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## average months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4800627-bf8d-4b2d-9b94-2033587dc1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density of average networks of month 1 is: 0.20750015690704826\n",
      "density of average networks of month 2 is: 0.14596774193548387\n",
      "density of average networks of month 3 is: 0.17672583051912355\n",
      "density of average networks of month 4 is: 0.2265123901556893\n",
      "density of average networks of month 5 is: 0.300997769072672\n",
      "density of average networks of month 6 is: 0.3147211474617692\n",
      "density of average networks of month 7 is: 0.3288832825256666\n",
      "density of average networks of month 8 is: 0.3285228519706723\n",
      "density of average networks of month 9 is: 0.3244558820946124\n",
      "density of average networks of month 10 is: 0.387579348947747\n",
      "density of average networks of month 11 is: 0.31912696918345573\n",
      "density of average networks of month 12 is: 0.23581246469591413\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for month in range(1, 13):\n",
    "\n",
    "    files = glob.glob(f'Processed/casuals/*/months/*network*-{month}.gml', recursive=True)\n",
    "    g_count = len(files)\n",
    "    \n",
    "    G_average = nx.DiGraph()\n",
    "    \n",
    "    for weight_name in ['trip_count_weights', 'duration_weights', 'tpd_weights']:\n",
    "    \n",
    "        c = Counter()\n",
    "        \n",
    "        for filename in files:\n",
    "        \n",
    "            G = nx.read_gml(filename)\n",
    "        \n",
    "            G_weights = dict(map(lambda x: ((x[0], x[1]), x[2]), G.edges.data(weight_name)))\n",
    "    \n",
    "            c.update(G_weights)\n",
    "    \n",
    "        edge_list = [(*x[0], {weight_name: x[1] / g_count}) for x in c.items()]\n",
    "    \n",
    "        G_average.add_edges_from(edge_list)\n",
    "\n",
    "    nx.write_gml(G_average, f'Processed/casuals/average/months/network-{month}.gml')\n",
    "\n",
    "    N, E = G_average.number_of_nodes(), G_average.number_of_edges()\n",
    "    print(f\"density of average networks of month {month} is: {E / (N * (N - 1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "60e24cf0-e3c5-4546-be5e-4b31d4d8d200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density of average networks of month 1 is: 0.20265102431685872\n",
      "density of average networks of month 2 is: 0.14291822186559028\n",
      "density of average networks of month 3 is: 0.17872672313461788\n",
      "density of average networks of month 4 is: 0.1961353687637289\n",
      "density of average networks of month 5 is: 0.22105021179095252\n",
      "density of average networks of month 6 is: 0.23279849449037895\n",
      "density of average networks of month 7 is: 0.24968068382786401\n",
      "density of average networks of month 8 is: 0.2521186278780098\n",
      "density of average networks of month 9 is: 0.25305859883809206\n",
      "density of average networks of month 10 is: 0.3290466327747442\n",
      "density of average networks of month 11 is: 0.2748164187535304\n",
      "density of average networks of month 12 is: 0.23020146864997176\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for month in range(1, 13):\n",
    "\n",
    "    files = glob.glob(f'Processed/members/*/months/*network*-{month}.gml', recursive=True)\n",
    "    g_count = len(files)\n",
    "    \n",
    "    G_average = nx.DiGraph()\n",
    "    \n",
    "    for weight_name in ['trip_count_weights', 'duration_weights', 'tpd_weights']:\n",
    "    \n",
    "        c = Counter()\n",
    "        \n",
    "        for filename in files:\n",
    "        \n",
    "            G = nx.read_gml(filename)\n",
    "        \n",
    "            G_weights = dict(map(lambda x: ((x[0], x[1]), x[2]), G.edges.data(weight_name)))\n",
    "    \n",
    "            c.update(G_weights)\n",
    "    \n",
    "        edge_list = [(*x[0], {weight_name: x[1] / g_count}) for x in c.items()]\n",
    "    \n",
    "        G_average.add_edges_from(edge_list)\n",
    "\n",
    "    nx.write_gml(G_average, f'Processed/members/average/months/network-{month}.gml')\n",
    "\n",
    "    N, E = G_average.number_of_nodes(), G_average.number_of_edges()\n",
    "    print(f\"density of average networks of month {month} is: {E / (N * (N - 1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a761a4-4711-4a7c-a49b-125b25f3c761",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## average temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f186b2c-a313-41e5-a627-4b3c6e85dd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density of average networks of temp interval 0 is: 0.0407578836456687\n",
      "density of average networks of temp interval 1 is: 0.1476444921922856\n",
      "density of average networks of temp interval 2 is: 0.23508589971753077\n",
      "density of average networks of temp interval 3 is: 0.2608874056242477\n",
      "density of average networks of temp interval 4 is: 0.3113203484177999\n",
      "density of average networks of temp interval 5 is: 0.2574102878070843\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import glob\n",
    "import networkx as nx\n",
    "\n",
    "for temp_int in range(6):\n",
    "\n",
    "    files = glob.glob(f'Processed/members/*/temp/*network*-{temp_int}.gml', recursive=True)\n",
    "    g_count = len(files)\n",
    "    \n",
    "    G_average = nx.DiGraph()\n",
    "    \n",
    "    for weight_name in ['trip_count_weights', 'duration_weights', 'tpd_weights']:\n",
    "    \n",
    "        c = Counter()\n",
    "        \n",
    "        for filename in files:\n",
    "        \n",
    "            G = nx.read_gml(filename)\n",
    "        \n",
    "            G_weights = dict(map(lambda x: ((x[0], x[1]), x[2]), G.edges.data(weight_name)))\n",
    "    \n",
    "            c.update(G_weights)\n",
    "    \n",
    "        edge_list = [(*x[0], {weight_name: x[1] / g_count}) for x in c.items()]\n",
    "    \n",
    "        G_average.add_edges_from(edge_list)\n",
    "\n",
    "    nx.write_gml(G_average, f'Processed/members/average/temp/network-temp-{temp_int}.gml')\n",
    "\n",
    "    N, E = G_average.number_of_nodes(), G_average.number_of_edges()\n",
    "    print(f\"density of average networks of temp interval {temp_int} is: {E / (N * (N - 1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31264f5d-25ba-4d3f-bdb0-6d2fb7c33c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density of average networks of temp interval 0 is: 0.01874932053611642\n",
      "density of average networks of temp interval 1 is: 0.10326321467098167\n",
      "density of average networks of temp interval 2 is: 0.25047985664504063\n",
      "density of average networks of temp interval 3 is: 0.3302109075192661\n",
      "density of average networks of temp interval 4 is: 0.4437219504903341\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import glob\n",
    "import networkx as nx\n",
    "\n",
    "for temp_int in range(6):\n",
    "\n",
    "    files = glob.glob(f'Processed/casuals/*/temp/*network*-{temp_int}.gml', recursive=True)\n",
    "    g_count = len(files)\n",
    "    \n",
    "    G_average = nx.DiGraph()\n",
    "    \n",
    "    for weight_name in ['trip_count_weights', 'duration_weights', 'tpd_weights']:\n",
    "    \n",
    "        c = Counter()\n",
    "        \n",
    "        for filename in files:\n",
    "        \n",
    "            G = nx.read_gml(filename)\n",
    "        \n",
    "            G_weights = dict(map(lambda x: ((x[0], x[1]), x[2]), G.edges.data(weight_name)))\n",
    "    \n",
    "            c.update(G_weights)\n",
    "    \n",
    "        edge_list = [(*x[0], {weight_name: x[1] / g_count}) for x in c.items()]\n",
    "    \n",
    "        G_average.add_edges_from(edge_list)\n",
    "\n",
    "    nx.write_gml(G_average, f'Processed/casuals/average/temp/network-temp-{temp_int}.gml')\n",
    "\n",
    "    N, E = G_average.number_of_nodes(), G_average.number_of_edges()\n",
    "    print(f\"density of average networks of temp interval {temp_int} is: {E / (N * (N - 1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a09e64-36c0-402c-b8c3-9634dd18f02c",
   "metadata": {},
   "source": [
    "## see self loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bea6adad-0f2a-4d8a-a7ee-e28f2b7ae8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob('Processed/**/average/months/*.gml', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "224537d6-097e-42b7-8e51-f9c90de9c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def find_selfloops(filename):\n",
    "    try:\n",
    "        G = nx.read_gml(filename)\n",
    "        self_loops = list(nx.selfloop_edges(G))\n",
    "        isolates = list(nx.isolates(G))\n",
    "        if len(self_loops) != 0 or len(isolates) != 0:\n",
    "            return filename\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "with mp.Pool() as pool:\n",
    "\n",
    "    results = pool.map(find_selfloops, files, chunksize=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fba1e54f-b07a-4497-85cf-7f0b28eec94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b2cb363c5545589a6c5c47283f6796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for filename in tqdm(files):\n",
    "    try:\n",
    "        G = nx.read_gml(filename)\n",
    "        G.remove_edges_from(list(nx.selfloop_edges(G)))\n",
    "        G.remove_nodes_from(list(nx.isolates(G)))\n",
    "        nx.write_gml(G, filename)\n",
    "    except Exception as e:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373a883-ed92-46e9-8274-6e9e181d34ad",
   "metadata": {},
   "source": [
    "## util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e9af3c-a91e-4e14-ad0f-f9c960efc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import networkx as nx\n",
    "\n",
    "def get_tempdf(df):\n",
    "    temp1_df = df[[\"Start Station Id\", \"Start Station Name\"]].rename({'Start Station Id':'Id', 'Start Station Name': 'Name'}, axis=1)\n",
    "    temp2_df = df[[\"End Station Id\", \"End Station Name\"]].rename({'End Station Id':'Id', 'End Station Name': 'Name'}, axis=1)\n",
    "    temp_df = pd.concat([temp1_df, temp2_df]).drop_duplicates()\n",
    "    return temp_df\n",
    "    \n",
    "def func2(df):\n",
    "    \n",
    "    whole_trips_df = df[['Start Time', 'End Time', 'Trip Id', 'Trip Duration', 'Start Station Id', 'End Station Id']]\n",
    "    whole_trips_df = whole_trips_df.astype({'Start Time': 'datetime64[ns]', 'End Time': 'datetime64[ns]'})\n",
    "\n",
    "    temp_df = whole_trips_df[['Start Time', 'End Time', 'Start Station Id', 'End Station Id', 'Trip Id']].groupby(['Start Station Id', 'End Station Id']).agg({'Trip Id': 'count', 'Start Time': 'min', 'End Time': 'max'}).reset_index().rename({'Trip Id': 'Trip Count'}, axis=1)\n",
    "    temp_df['Link Duration'] = temp_df.apply(lambda x: (x['End Time'] - x['Start Time']).ceil('d'), axis=1).dt.days    #.astype('timedelta64[D]')\n",
    "    temp_df['Avg Duration'] = whole_trips_df[['Start Station Id', 'End Station Id', 'Trip Duration']].groupby(['Start Station Id', 'End Station Id']).mean().reset_index()['Trip Duration'] # why using trip duration and not link duration\n",
    "    temp_df['duration_weights'] = temp_df['Avg Duration'].apply(lambda x: 1 / x)\n",
    "\n",
    "    temp_df['tpd_weights'] = temp_df.apply(lambda x: x['Trip Count'] / x['Link Duration'], axis=1)\n",
    "    temp_df = temp_df.rename({'Trip Count': 'trip_count_weights'}, axis=1).drop(['Start Time', 'End Time', 'Link Duration', 'Avg Duration'], axis=1)\n",
    "\n",
    "    nnp = np.percentile(temp_df[['trip_count_weights', 'duration_weights', 'tpd_weights']], 99, axis=0)\n",
    "    temp_df[['trip_count_weights', 'duration_weights', 'tpd_weights']] = np.clip(temp_df[['trip_count_weights', 'duration_weights', 'tpd_weights']], None, nnp)\n",
    "\n",
    "    min_max_scaler = MinMaxScaler((1, 10))\n",
    "    x_scaled = min_max_scaler.fit_transform(temp_df[['trip_count_weights', 'duration_weights', 'tpd_weights']])\n",
    "    temp_df[['trip_count_weights', 'duration_weights', 'tpd_weights']] = x_scaled\n",
    "\n",
    "    G = nx.from_pandas_edgelist(temp_df, source=\"Start Station Id\", target=\"End Station Id\", edge_attr=['trip_count_weights', 'duration_weights', 'tpd_weights'], create_using=nx.DiGraph)\n",
    "\n",
    "    node_labels = {d['Id']: d['Name'] for d in get_tempdf(df)[['Id', 'Name']].to_dict('records')}\n",
    "    nx.set_node_attributes(G, node_labels, name='Name')\n",
    "\n",
    "    return G, temp_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
