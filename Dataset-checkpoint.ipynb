{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de176ddc-1736-4555-abcf-a8589e71cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "777ac039-5e3a-486f-b29f-b5aa2cefc1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob('Data/**/*.csv', recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1bd9a3a-2bee-41d5-9b52-0b79c0c5055a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files) # 2017 (4), 2018 (4), 2019 (4), 2020 (12), 2021 (12), 2022 (12), 2023 (12), 2024 (9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23163957-95bd-4c9f-9b5b-b2a6a86288e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mapping = {'Bike Id': 'Bike Id',\n",
    " 'End Station Id': 'End Station Id',\n",
    " 'End Station Name': 'End Station Name',\n",
    " 'End Time': 'End Time',\n",
    " 'Model': 'Model',\n",
    " 'Start Station Id': 'Start Station Id',\n",
    " 'Start Station Name': 'Start Station Name',\n",
    " 'Start Time': 'Start Time',\n",
    " 'Trip  Duration': 'Trip Duration',\n",
    " 'Trip Id': 'Trip Id',\n",
    " 'User Type': 'User Type',\n",
    " 'from_station_id': 'Start Station Id',\n",
    " 'from_station_name': 'Start Station Name',\n",
    " 'to_station_id': 'End Station Id',\n",
    " 'to_station_name': 'End Station Name',\n",
    " 'trip_duration_seconds': 'Trip Duration',\n",
    " 'trip_id': 'Trip Id',\n",
    " 'trip_start_time': 'Start Time',\n",
    " 'trip_stop_time': 'End Time',\n",
    " 'user_type': 'User Type'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1129de2-0415-4412-8f7f-712bfd35a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_types = {'Trip Id': 'str',\n",
    "                 'Start Time': 'datetime64[ns]', \n",
    "                 'End Time': 'datetime64[ns]', \n",
    "                 'Trip Duration': 'int32', \n",
    "                 'Start Station Id': 'int32', \n",
    "                 'Start Station Name': 'str', \n",
    "                 'End Station Id': 'int32', \n",
    "                 'End Station Name': 'str', \n",
    "                 'User Type': 'str'}\n",
    "\n",
    "incomplete_columns_types = {'Trip Id': 'str',\n",
    "                 'Start Time': 'datetime64[ns]', \n",
    "                 'End Time': 'datetime64[ns]', \n",
    "                 'Trip Duration': 'int32', \n",
    "                 'Start Station Name': 'str', \n",
    "                 'End Station Name': 'str', \n",
    "                 'User Type': 'str'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa33eda3-1a66-44e1-9003-7f243bd2f1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdee9c08c490431c90af1b6fe214694c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/bikeshare-ridership-2017/2017 Data/Bikeshare Ridership (2017 Q3).csv\n",
      "Data/bikeshare-ridership-2017/2017 Data/Bikeshare Ridership (2017 Q4).csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Read, clean, and store each dataset in a list\n",
    "final_dfs = []\n",
    "skipped_files = []    \n",
    "\n",
    "for file in tqdm(files[:4]):\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='utf-8')\n",
    "        df_cleaned = df.dropna()  # Remove null values\n",
    "        df_renamed = df_cleaned.rename(columns_mapping, axis=1) # Rename columns if necessary\n",
    "        df_corrected = df_renamed.astype(columns_types)\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            df = pd.read_csv(file, encoding='cp1252')\n",
    "            df_cleaned = df.dropna()  # Remove null values\n",
    "            df_renamed = df_cleaned.rename(columns_mapping, axis=1) # Rename columns if necessary\n",
    "            df_corrected = df_renamed.astype(columns_types)\n",
    "        except Exception as e:\n",
    "            print(file)\n",
    "    finally:\n",
    "        final_dfs.append(df_corrected)\n",
    "\n",
    "# Merge all datasets vertically\n",
    "merged_df = pd.concat(final_dfs, join='inner', ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b50a1654-a2ad-497c-9168-6a8fc3df87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def do_the_work(filename):\n",
    "    try:\n",
    "        df = pd.read_csv(filename, encoding='utf-8')\n",
    "        df_cleaned = df.dropna()  # Remove null values\n",
    "        df_renamed = df_cleaned.rename(columns_mapping, axis=1) # Rename columns if necessary\n",
    "        df_corrected = df_renamed.astype(columns_types)\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            df = pd.read_csv(filename, encoding='cp1252')\n",
    "            df_cleaned = df.dropna()  # Remove null values\n",
    "            df_renamed = df_cleaned.rename(columns_mapping, axis=1) # Rename columns if necessary\n",
    "            df_corrected = df_renamed.astype(columns_types)\n",
    "        except Exception as e:\n",
    "            df_corrected = None\n",
    "    finally:\n",
    "        return df_corrected\n",
    "\n",
    "# master_list = [file1, file2, fil3, ...filen]\n",
    "with mp.Pool() as pool:\n",
    "    results = pool.map(do_the_work, files, chunksize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "725ff3f2-bb4b-4649-807a-ea135a1fce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dfs = [df for df in results if df is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0be5bd30-c0ed-4c8f-be7a-842a8c6dd371",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat(final_dfs, join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ab2fe8-d3dc-47c3-9899-6cc74e066369",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### fix 2017 Q3 Q4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fd8c8cf-c59a-4076-b423-fe6567b37089",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extra dfs\n",
    "\n",
    "leftout_files = [\"Data/bikeshare-ridership-2017/2017 Data/Bikeshare Ridership (2017 Q3).csv\" \n",
    "                ,\"Data/bikeshare-ridership-2017/2017 Data/Bikeshare Ridership (2017 Q4).csv\"]\n",
    "\n",
    "leftout_dfs = []\n",
    "\n",
    "for file in leftout_files:\n",
    "\n",
    "    df = pd.read_csv(file, encoding='utf-8')\n",
    "    df_cleaned = df.dropna()  # Remove null values\n",
    "    df_renamed = df_cleaned.rename(columns_mapping, axis=1) # Rename columns if necessary\n",
    "    df_corrected = df_renamed.astype(incomplete_columns_types, errors='ignore')\n",
    "    leftout_dfs.append(df_corrected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ea48656-c5b2-40e2-8177-305d07959861",
   "metadata": {},
   "outputs": [],
   "source": [
    "leftout_merged_df = pd.concat(leftout_dfs, join='inner', ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fea1df2-d602-4935-bcd3-2f653290369e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trip Id</th>\n",
       "      <th>Start Time</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Trip Duration</th>\n",
       "      <th>Start Station Name</th>\n",
       "      <th>End Station Name</th>\n",
       "      <th>User Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1253914</td>\n",
       "      <td>2017-07-01 00:00:00</td>\n",
       "      <td>2017-07-01 00:15:00</td>\n",
       "      <td>910</td>\n",
       "      <td>Princess St / Adelaide St E</td>\n",
       "      <td>424 Wellington St W</td>\n",
       "      <td>Member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1253915</td>\n",
       "      <td>2017-07-01 00:01:00</td>\n",
       "      <td>2017-07-01 00:15:00</td>\n",
       "      <td>837</td>\n",
       "      <td>Fort York  Blvd / Capreol Crt</td>\n",
       "      <td>HTO Park (Queens Quay W)</td>\n",
       "      <td>Casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1253916</td>\n",
       "      <td>2017-07-01 00:01:00</td>\n",
       "      <td>2017-07-01 00:14:00</td>\n",
       "      <td>786</td>\n",
       "      <td>Fort York  Blvd / Capreol Crt</td>\n",
       "      <td>HTO Park (Queens Quay W)</td>\n",
       "      <td>Casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1253917</td>\n",
       "      <td>2017-07-01 00:01:00</td>\n",
       "      <td>2017-07-01 00:25:00</td>\n",
       "      <td>1420</td>\n",
       "      <td>Elizabeth St / Edward St (Bus Terminal)</td>\n",
       "      <td>Boston Ave / Queen St E</td>\n",
       "      <td>Casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1253918</td>\n",
       "      <td>2017-07-01 00:01:00</td>\n",
       "      <td>2017-07-01 00:25:00</td>\n",
       "      <td>1437</td>\n",
       "      <td>Elizabeth St / Edward St (Bus Terminal)</td>\n",
       "      <td>Boston Ave / Queen St E</td>\n",
       "      <td>Casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026887</th>\n",
       "      <td>2383642</td>\n",
       "      <td>2017-12-31 23:46:27</td>\n",
       "      <td>2017-12-31 23:46:53</td>\n",
       "      <td>26</td>\n",
       "      <td>Bloor St / Brunswick Ave</td>\n",
       "      <td>Bloor St / Brunswick Ave</td>\n",
       "      <td>Casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026888</th>\n",
       "      <td>2383643</td>\n",
       "      <td>2017-12-31 23:47:13</td>\n",
       "      <td>2018-01-01 00:11:40</td>\n",
       "      <td>1467</td>\n",
       "      <td>Bloor St / Brunswick Ave</td>\n",
       "      <td>HTO Park (Queens Quay W)</td>\n",
       "      <td>Casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026889</th>\n",
       "      <td>2383644</td>\n",
       "      <td>2017-12-31 23:47:40</td>\n",
       "      <td>2017-12-31 23:57:49</td>\n",
       "      <td>609</td>\n",
       "      <td>Kendal Ave / Spadina Rd</td>\n",
       "      <td>Augusta Ave / Denison Sq</td>\n",
       "      <td>Member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026890</th>\n",
       "      <td>2383645</td>\n",
       "      <td>2017-12-31 23:49:08</td>\n",
       "      <td>2017-12-31 23:49:34</td>\n",
       "      <td>26</td>\n",
       "      <td>Phoebe St / Spadina Ave</td>\n",
       "      <td>Phoebe St / Spadina Ave</td>\n",
       "      <td>Member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026891</th>\n",
       "      <td>2383646</td>\n",
       "      <td>2017-12-31 23:49:41</td>\n",
       "      <td>2017-12-31 23:57:41</td>\n",
       "      <td>480</td>\n",
       "      <td>Phoebe St / Spadina Ave</td>\n",
       "      <td>Simcoe St / Queen St W</td>\n",
       "      <td>Member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1026892 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Trip Id          Start Time            End Time  Trip Duration  \\\n",
       "0        1253914 2017-07-01 00:00:00 2017-07-01 00:15:00            910   \n",
       "1        1253915 2017-07-01 00:01:00 2017-07-01 00:15:00            837   \n",
       "2        1253916 2017-07-01 00:01:00 2017-07-01 00:14:00            786   \n",
       "3        1253917 2017-07-01 00:01:00 2017-07-01 00:25:00           1420   \n",
       "4        1253918 2017-07-01 00:01:00 2017-07-01 00:25:00           1437   \n",
       "...          ...                 ...                 ...            ...   \n",
       "1026887  2383642 2017-12-31 23:46:27 2017-12-31 23:46:53             26   \n",
       "1026888  2383643 2017-12-31 23:47:13 2018-01-01 00:11:40           1467   \n",
       "1026889  2383644 2017-12-31 23:47:40 2017-12-31 23:57:49            609   \n",
       "1026890  2383645 2017-12-31 23:49:08 2017-12-31 23:49:34             26   \n",
       "1026891  2383646 2017-12-31 23:49:41 2017-12-31 23:57:41            480   \n",
       "\n",
       "                              Start Station Name          End Station Name  \\\n",
       "0                    Princess St / Adelaide St E       424 Wellington St W   \n",
       "1                  Fort York  Blvd / Capreol Crt  HTO Park (Queens Quay W)   \n",
       "2                  Fort York  Blvd / Capreol Crt  HTO Park (Queens Quay W)   \n",
       "3        Elizabeth St / Edward St (Bus Terminal)   Boston Ave / Queen St E   \n",
       "4        Elizabeth St / Edward St (Bus Terminal)   Boston Ave / Queen St E   \n",
       "...                                          ...                       ...   \n",
       "1026887                 Bloor St / Brunswick Ave  Bloor St / Brunswick Ave   \n",
       "1026888                 Bloor St / Brunswick Ave  HTO Park (Queens Quay W)   \n",
       "1026889                  Kendal Ave / Spadina Rd  Augusta Ave / Denison Sq   \n",
       "1026890                  Phoebe St / Spadina Ave   Phoebe St / Spadina Ave   \n",
       "1026891                  Phoebe St / Spadina Ave    Simcoe St / Queen St W   \n",
       "\n",
       "        User Type  \n",
       "0          Member  \n",
       "1          Casual  \n",
       "2          Casual  \n",
       "3          Casual  \n",
       "4          Casual  \n",
       "...           ...  \n",
       "1026887    Casual  \n",
       "1026888    Casual  \n",
       "1026889    Member  \n",
       "1026890    Member  \n",
       "1026891    Member  \n",
       "\n",
       "[1026892 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leftout_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e2d2f2c-4bbd-454a-b3ad-327e1c43118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "leftout_stations = set(leftout_merged_df['Start Station Name'].unique()).union(set(leftout_merged_df['End Station Name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5470a185-0ff5-467c-9bd4-807288099270",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stations = set(merged_df['Start Station Name'].unique()).union(set(merged_df['End Station Name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bbb74d7-fef2-49a6-a64d-6c3d7f6cbea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "remaining_stations = np.array([*(leftout_stations - other_stations)])\n",
    "other_stations = np.array([*other_stations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04b1a9a9-109d-416b-b73b-9a1c761bec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Threshold.similarity import similar\n",
    "import numpy as np\n",
    "\n",
    "matched = []\n",
    "\n",
    "similarity_matrix = np.zeros((len(remaining_stations), len(other_stations)))\n",
    "\n",
    "for i in range(len(remaining_stations)):\n",
    "    for j in range(len(other_stations)):\n",
    "        similarity_matrix[i, j] = similar(remaining_stations[i], other_stations[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb7cc56a-06c6-43b0-8806-a10cfd59a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argpartition(similarity_matrix, -4, axis=1)[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b89e6027-5b77-4d0d-9838-69c920c85056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lansdowne Subway Green P [('Woodbine Subway Green P SMART', 0.6792452830188679), ('Lansdowne Subway Station', 0.75), ('Landsdowne Subway Green P', 0.9795918367346939), ('Pape Subway Green P', 0.7906976744186046)]\n",
      "**********\n",
      "Michael Sweet Ave / St. Patrick St [('Simcoe St / Michael Sweet Ave', 0.5396825396825397), ('Mutual St / Shuter St', 0.5454545454545454), ('Dundas St W / St. Patrick St', 0.7096774193548387), ('Simcoe St / Michael Sweet Ave - SMART', 0.5633802816901409)]\n",
      "**********\n",
      "Base Station [('Castle Frank Station', 0.6875), ('Rosedale Station', 0.7142857142857143), ('Chester Station', 0.7407407407407407), ('Donlands Station', 0.7142857142857143)]\n",
      "**********\n",
      "Margueretta St / College St [('St Clarens Ave / College St', 0.6296296296296297), ('Beverly St / College St W', 0.6923076923076923), ('Beverley St / College St', 0.7450980392156863), ('Margueretta St / College St W', 0.9642857142857143)]\n",
      "**********\n",
      "Fringe Next Stage - 7219 [('King St W / Stafford St', 0.425531914893617), ('Yonge St / Alexander St - SMART', 0.43636363636363634), ('Yonge St / Merton St ', 0.4444444444444444), ('Princess St / Adelaide St E', 0.43137254901960786)]\n",
      "**********\n",
      "Lake Shore Blvd W / Ontario Dr(Ontario Place) [('Lake Shore Blvd W / Seventeenth St', 0.6329113924050633), ('Lake Shore Blvd W / Brow Dr', 0.6944444444444444), ('Lakeshore Blvd W / Ontario Dr', 0.7567567567567568), ('Lake Shore Blvd W / Ontario Dr', 0.8)]\n",
      "**********\n",
      "Dovercourt Rd / Harrison St - SMART [('Blythwood Rd / Yonge St - SMART', 0.6363636363636364), ('Davenport Rd / Oakwood Rd - SMART', 0.6764705882352942), ('Dovercourt Rd / Harrison St (Green P) - SMART', 0.875), ('Davenport Rd / McAlpine St - SMART', 0.7246376811594203)]\n",
      "**********\n",
      "Roxton Rd / College St [('St Clarens Ave / College St', 0.6530612244897959), ('Ossington Ave / College St W', 0.68), ('Roxton Rd / Harbord St', 0.7272727272727273), ('Ossington Ave / College St', 0.7083333333333334)]\n",
      "**********\n",
      "Summerhill Ave / MacLennan Ave - SMART [('Wright Ave / Sorauren Ave - SMART', 0.676056338028169), ('Fulton Ave / Pape Ave - SMART', 0.6865671641791045), ('Rosehill Ave / Avoca Ave - SMART', 0.7142857142857143), ('Summerhill Ave / Maclennan Ave', 0.8529411764705882)]\n",
      "**********\n",
      "Beverly St / College St [('Margueretta St / College St W', 0.6923076923076923), ('University Ave / College St', 0.72), ('Beverley St / College St', 0.9787234042553191), ('Beverly St / College St W', 0.9583333333333334)]\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(remaining_stations)):\n",
    "    print(remaining_stations[i], list(zip(other_stations[inds[i]], similarity_matrix[i, inds[i]])))\n",
    "    print(\"*\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96a7c101-9a55-499f-aadd-045eba88c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_changes = {\"Dovercourt Rd / Harrison St - SMART\": 'Dovercourt Rd / Harrison St (Green P) - SMART',\n",
    " \"Lake Shore Blvd W / Ontario Dr(Ontario Place)\": 'Lake Shore Blvd W / Ontario Dr',\n",
    " \"Lansdowne Subway Green P\": 'Landsdowne Subway Green P',\n",
    " \"Margueretta St / College St\": 'Margueretta St / College St W',\n",
    " \"Summerhill Ave / MacLennan Ave - SMART\": 'Summerhill Ave / Maclennan Ave',\n",
    " \"Roxton Rd / College St\": 'Ossington Ave / College St',\n",
    " \"Michael Sweet Ave / St. Patrick St\": 'Dundas St W / St. Patrick St',\n",
    " \"Beverly St / College St\": 'Beverley St / College St'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74933511-2afe-45c8-8029-bdcb67725f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of 'Base Station'\n",
    "\n",
    "leftout_merged_df = leftout_merged_df[(leftout_merged_df[\"Start Station Name\"] != \"Base Station\") & (leftout_merged_df[\"End Station Name\"] != \"Base Station\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1dacba2-18ba-4598-b475-6ee9e0244c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for change in name_changes.items():\n",
    "\n",
    "    leftout_merged_df.loc[leftout_merged_df['Start Station Name'] == change[0], 'Start Station Name'] = change[1]\n",
    "\n",
    "    leftout_merged_df.loc[leftout_merged_df['End Station Name'] == change[0], 'End Station Name'] = change[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "983b7f02-7730-4ee9-bb26-097a2c39d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "leftout_stations = set(leftout_merged_df['Start Station Name'].unique()).union(set(leftout_merged_df['End Station Name'].unique()))\n",
    "other_stations = set(merged_df['Start Station Name'].unique()).union(set(merged_df['End Station Name'].unique()))\n",
    "\n",
    "remaining_stations = np.array([*(leftout_stations - other_stations)])\n",
    "other_stations = np.array([*other_stations])\n",
    "\n",
    "# replacing Fringe Next Stage - 7219 with King St / Victoria St\n",
    "remaining_stations[remaining_stations == \"Fringe Next Stage - 7219\"] = \"King St / Victoria St\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa9a2b20-f075-46c3-8f7a-a052aaaf1705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['King St / Victoria St'], dtype='<U24')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e185dccc-fa13-48a7-bc78-8bf2d0e8e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Threshold.similarity import similar\n",
    "import numpy as np\n",
    "\n",
    "matched = []\n",
    "\n",
    "similarity_matrix = np.zeros((len(remaining_stations), len(other_stations)))\n",
    "\n",
    "for i in range(len(remaining_stations)):\n",
    "    for j in range(len(other_stations)):\n",
    "        similarity_matrix[i, j] = similar(remaining_stations[i], other_stations[j])\n",
    "\n",
    "inds = np.argpartition(similarity_matrix, -4, axis=1)[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a2e6c3a-6a83-4d55-8f2b-e1bce2675b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King St / Victoria St [('King St W / York St', 0.75), ('King St W / Jordan St', 0.7619047619047619), ('King St E / Victoria St', 0.9545454545454546), ('Dundas St E / Victoria St', 0.782608695652174)]\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(remaining_stations)):\n",
    "    print(remaining_stations[i], list(zip(other_stations[inds[i]], similarity_matrix[i, inds[i]])))\n",
    "    print(\"*\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbc8f1c0-ce8c-41f4-bf69-ea106d6aeb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name_changes = {\"Fringe Next Stage - 7219\": 'King St E / Victoria St'}\n",
    "\n",
    "for change in new_name_changes.items():\n",
    "\n",
    "    leftout_merged_df.loc[leftout_merged_df['Start Station Name'] == change[0], 'Start Station Name'] = change[1]\n",
    "\n",
    "    leftout_merged_df.loc[leftout_merged_df['End Station Name'] == change[0], 'End Station Name'] = change[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37e2f876-44ee-4552-ad1d-ae63b581bf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leftout_stations = set(leftout_merged_df['Start Station Name'].unique()).union(set(leftout_merged_df['End Station Name'].unique()))\n",
    "other_stations = set(merged_df['Start Station Name'].unique()).union(set(merged_df['End Station Name'].unique()))\n",
    "\n",
    "remaining_stations = np.array([*(leftout_stations - other_stations)])\n",
    "remaining_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8c785d8-6569-4455-a616-993e72fb2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "leftout_merged_df = leftout_merged_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b97bc432-8e3b-44b2-8242-27e0122fa8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "leftout_merged_df['Start Station Id'] = pd.merge(temp_df, leftout_merged_df, left_on='Name', right_on='Start Station Name', how='right')['Id'].astype('int32')\n",
    "leftout_merged_df['End Station Id'] = pd.merge(temp_df, leftout_merged_df, left_on='Name', right_on='End Station Name', how='right')['Id'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a1ade62-9c77-471f-aeea-edb59da220c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "leftout_merged_df.to_csv('2017_Q3_Q4_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25840f19-726c-44b4-be6a-646ef06000aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1026889"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leftout_merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0cb3bc29-486c-4bb0-bb9d-9ef1c81b7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([merged_df, leftout_merged_df], join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b496814a-ceeb-45de-8177-75da4bff3db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26321056"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab959c8b-1aa7-4838-8b00-32409d040186",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('all_trips.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79b54f-e831-408a-b71c-a7fcf073f6ee",
   "metadata": {},
   "source": [
    "#### cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08c813f7-379d-43fe-be62-19f155e85b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Threshold.similarity import compute_avg_similarity\n",
    "import random\n",
    "\n",
    "def get_tempdf(df):\n",
    "    temp1_df = df[[\"Start Station Id\", \"Start Station Name\"]].rename({'Start Station Id':'Id', 'Start Station Name': 'Name'}, axis=1)\n",
    "    temp2_df = df[[\"End Station Id\", \"End Station Name\"]].rename({'End Station Id':'Id', 'End Station Name': 'Name'}, axis=1)\n",
    "    temp_df = pd.concat([temp1_df, temp2_df]).drop_duplicates()\n",
    "    return temp_df\n",
    "\n",
    "def change_membership(membership):\n",
    "    if membership == 'Annual Member':\n",
    "        return 'Member'\n",
    "    elif membership == 'Casual Member':\n",
    "        return 'Casual'\n",
    "    else:\n",
    "        return membership\n",
    "        \n",
    "def func1(df):\n",
    "    temp_df = get_tempdf(df)\n",
    "    temp_temp_df = (temp_df.groupby(\"Id\").count() > 1).reset_index()\n",
    "\n",
    "\n",
    "    threshold = 0.33\n",
    "    changes = []\n",
    "    \n",
    "    # we are trying to find the mapping and after we've found one, we replace the duplicates\n",
    "    for id in temp_temp_df[temp_temp_df['Name'] == True].Id.to_numpy():\n",
    "        duplicates = temp_df[temp_df['Id'] == id]\n",
    "        size = len(duplicates)\n",
    "    \n",
    "        similarity = compute_avg_similarity(duplicates.Name.to_numpy())\n",
    "    \n",
    "        if similarity > threshold:\n",
    "            choice = size - 1\n",
    "        else:\n",
    "            choice = size\n",
    "        \n",
    "        # if any of the choices -> add a tuple for every other choice\n",
    "        # else do nothing\n",
    "        if choice < size:\n",
    "            \n",
    "            choice_name = duplicates.iloc[choice, 1]\n",
    "    \n",
    "            for i in range(size):\n",
    "                if i == choice:\n",
    "                    continue\n",
    "                changes.append((duplicates.iloc[i, 1], choice_name))\n",
    "\n",
    "\n",
    "    for change in changes:\n",
    "        \n",
    "        df.loc[df['Start Station Name'] == change[0], 'Start Station Name'] = change[1]\n",
    "    \n",
    "        df.loc[df['End Station Name'] == change[0], 'End Station Name'] = change[1]\n",
    "\n",
    "    temp_df = get_tempdf(df)\n",
    "    new_changes = []\n",
    "\n",
    "    for id in [7117, 7130, 7291, 7381]:\n",
    "        duplicates = temp_df[temp_df['Id'] == id]\n",
    "        size = len(duplicates)\n",
    "\n",
    "        if size == 0:\n",
    "            continue\n",
    "    \n",
    "        choice = size - 1\n",
    "            \n",
    "        choice_name = duplicates.iloc[choice, 1]\n",
    "    \n",
    "        for i in range(size):\n",
    "            if i == choice:\n",
    "                continue\n",
    "            new_changes.append((duplicates.iloc[i, 1], choice_name))\n",
    "\n",
    "\n",
    "    comb_to_change = [((7001, \"Lower Jarvis St / The Esplanade\"), 7686)]\n",
    "\n",
    "    for change in comb_to_change:\n",
    "    \n",
    "        df.loc[(df['Start Station Id'] == change[0][0]) & (df['Start Station Name'] == change[0][1]), 'Start Station Id'] = change[1]\n",
    "    \n",
    "        df.loc[(df['End Station Id'] == change[0][0]) & (df['End Station Name'] == change[0][1]), 'End Station Id'] = change[1]\n",
    "\n",
    "    id_to_change = [(7720, 7372), (7685, 7322), (7712, 7072), (7680, 7510), (7792, 7098), (7470, 7398)]\n",
    "    \n",
    "    for change in id_to_change:\n",
    "    \n",
    "        df.loc[df['Start Station Id'] == change[0], 'Start Station Id'] = change[1]\n",
    "    \n",
    "        df.loc[df['End Station Id'] == change[0], 'End Station Id'] = change[1]\n",
    "\n",
    "    available_ids = list(set(range(7000,8000)) - set(temp_df['Id'].unique()))\n",
    "\n",
    "    new_id_changes = []\n",
    "    \n",
    "    temp_temp_df = (temp_df.groupby(\"Id\").count() > 1).reset_index()\n",
    "    \n",
    "    for id in temp_temp_df[temp_temp_df['Name'] == True].Id.to_numpy():\n",
    "    \n",
    "        # choose one of the options to change id for\n",
    "        duplicates = temp_df[temp_df['Id'] == id]\n",
    "        size = len(duplicates)\n",
    "    \n",
    "        choice = size - 1 \n",
    "        choice_name = duplicates.iloc[choice, 1]\n",
    "    \n",
    "        new_id = random.choice(available_ids)\n",
    "        available_ids.remove(new_id)\n",
    "    \n",
    "        new_id_changes.append(((id, choice_name), new_id))\n",
    "\n",
    "    for id_change in new_id_changes:\n",
    "    \n",
    "        df.loc[(df['Start Station Id'] == id_change[0][0]) & (df['Start Station Name'] == id_change[0][1]), 'Start Station Id'] = id_change[1]\n",
    "    \n",
    "        df.loc[(df['End Station Id'] == id_change[0][0]) & (df['End Station Name'] == id_change[0][1]), 'End Station Id'] = id_change[1]\n",
    "\n",
    "    df['User Type'] = df['User Type'].apply(change_membership)\n",
    "\n",
    "    df = df[df['Trip Duration'] > 29]\n",
    "\n",
    "    df = df[df['Start Station Id'] != df['End Station Id']]\n",
    "\n",
    "    df = df[df['Start Time'] < df['End Time']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "318d7a4a-8576-4c4e-843d-992ba0edf734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_stations(df):\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    with open('geocoding/grouping/proximity_mapping_edited.pkl', 'rb') as f:\n",
    "        mapping = pickle.load(f)\n",
    "\n",
    "    for source_id, (dest_id, dest_name) in mapping.items():\n",
    "        df.loc[df['Start Station Id'] == source_id, ['Start Station Id', 'Start Station Name']] = [dest_id, dest_name]\n",
    "        df.loc[df['End Station Id'] == source_id, ['End Station Id', 'End Station Name']] = [dest_id, dest_name]\n",
    "\n",
    "    df = df[df['Start Station Id'] != df['End Station Id']] # this should solve the self loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c369545-97a2-44bf-8f8a-6450081aba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = func1(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e6b1c0eb-27dd-4ee6-982c-f314272f678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_csv('cleaned_trips.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5f2a77af-d244-419e-a58f-c8f60cd722ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = get_tempdf(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0ce793b-76ba-40ba-bb12-3b5cbecd7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv('stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e2df5471-dd76-405c-aa4e-b443316e26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_stations(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c9af7a45-36bf-4769-888d-95527fcbdcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_csv('grp_stations_trips.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6b8e36c3-16f7-47c8-bd66-a2c59eec68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = get_tempdf(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c3bd99cc-51db-42f4-a038-483d34b6fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv('grp_stations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0022f18d-2620-4098-a1b8-04ba0af3bb9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### whole trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ad73d5-e80d-444e-93dc-a421ab75599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_trips_df = merged_df[['Start Time', 'End Time', 'Trip Id', 'Trip Duration', 'Start Station Id', 'End Station Id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14055e89-e21e-46b6-af78-5d6332ea25bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_trips_df = whole_trips_df.astype({'Start Time': 'datetime64[ns]', 'End Time': 'datetime64[ns]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "869a83d7-8105-47a4-8b48-61593b308497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start Time</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Trip Id</th>\n",
       "      <th>Trip Duration</th>\n",
       "      <th>Start Station Id</th>\n",
       "      <th>End Station Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:03:00</td>\n",
       "      <td>712382</td>\n",
       "      <td>223</td>\n",
       "      <td>7051</td>\n",
       "      <td>7089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:05:00</td>\n",
       "      <td>712383</td>\n",
       "      <td>279</td>\n",
       "      <td>7143</td>\n",
       "      <td>7154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 00:05:00</td>\n",
       "      <td>2017-01-01 00:29:00</td>\n",
       "      <td>712384</td>\n",
       "      <td>1394</td>\n",
       "      <td>7113</td>\n",
       "      <td>7199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 00:07:00</td>\n",
       "      <td>2017-01-01 00:21:00</td>\n",
       "      <td>712385</td>\n",
       "      <td>826</td>\n",
       "      <td>7077</td>\n",
       "      <td>7010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 00:08:00</td>\n",
       "      <td>2017-01-01 00:12:00</td>\n",
       "      <td>712386</td>\n",
       "      <td>279</td>\n",
       "      <td>7079</td>\n",
       "      <td>7047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25244019</th>\n",
       "      <td>2017-12-31 23:37:43</td>\n",
       "      <td>2017-12-31 23:43:00</td>\n",
       "      <td>2383640</td>\n",
       "      <td>317</td>\n",
       "      <td>7152</td>\n",
       "      <td>7184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25244020</th>\n",
       "      <td>2017-12-31 23:42:06</td>\n",
       "      <td>2017-12-31 23:58:24</td>\n",
       "      <td>2383641</td>\n",
       "      <td>978</td>\n",
       "      <td>7012</td>\n",
       "      <td>7184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25244021</th>\n",
       "      <td>2017-12-31 23:47:13</td>\n",
       "      <td>2018-01-01 00:11:40</td>\n",
       "      <td>2383643</td>\n",
       "      <td>1467</td>\n",
       "      <td>7061</td>\n",
       "      <td>7175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25244022</th>\n",
       "      <td>2017-12-31 23:47:40</td>\n",
       "      <td>2017-12-31 23:57:49</td>\n",
       "      <td>2383644</td>\n",
       "      <td>609</td>\n",
       "      <td>7132</td>\n",
       "      <td>7189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25244023</th>\n",
       "      <td>2017-12-31 23:49:41</td>\n",
       "      <td>2017-12-31 23:57:41</td>\n",
       "      <td>2383646</td>\n",
       "      <td>480</td>\n",
       "      <td>7020</td>\n",
       "      <td>7022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25244024 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Start Time            End Time  Trip Id  Trip Duration  \\\n",
       "0        2017-01-01 00:00:00 2017-01-01 00:03:00   712382            223   \n",
       "1        2017-01-01 00:00:00 2017-01-01 00:05:00   712383            279   \n",
       "2        2017-01-01 00:05:00 2017-01-01 00:29:00   712384           1394   \n",
       "3        2017-01-01 00:07:00 2017-01-01 00:21:00   712385            826   \n",
       "4        2017-01-01 00:08:00 2017-01-01 00:12:00   712386            279   \n",
       "...                      ...                 ...      ...            ...   \n",
       "25244019 2017-12-31 23:37:43 2017-12-31 23:43:00  2383640            317   \n",
       "25244020 2017-12-31 23:42:06 2017-12-31 23:58:24  2383641            978   \n",
       "25244021 2017-12-31 23:47:13 2018-01-01 00:11:40  2383643           1467   \n",
       "25244022 2017-12-31 23:47:40 2017-12-31 23:57:49  2383644            609   \n",
       "25244023 2017-12-31 23:49:41 2017-12-31 23:57:41  2383646            480   \n",
       "\n",
       "          Start Station Id  End Station Id  \n",
       "0                     7051            7089  \n",
       "1                     7143            7154  \n",
       "2                     7113            7199  \n",
       "3                     7077            7010  \n",
       "4                     7079            7047  \n",
       "...                    ...             ...  \n",
       "25244019              7152            7184  \n",
       "25244020              7012            7184  \n",
       "25244021              7061            7175  \n",
       "25244022              7132            7189  \n",
       "25244023              7020            7022  \n",
       "\n",
       "[25244024 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_trips_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "17e5efa5-1480-4930-925b-83906fe2e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48732/638465002.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  whole_trips_df.loc['Scaled Duration'] = np.clip(whole_trips_df['Trip Duration'], None, upper_limit)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set upper limit to the 99th percentile\n",
    "upper_limit = np.percentile(whole_trips_df['Trip Duration'], 99)\n",
    "whole_trips_df['Scaled Duration'] = np.clip(whole_trips_df['Trip Duration'], None, upper_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e524707a-61ea-4fbd-b3db-d45a29357548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48732/3718487274.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  whole_trips_df['Scaled Duration'] = scaler.fit_transform(whole_trips_df['Scaled Duration'].values.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "# Normalize using Min-Max Scaling\n",
    "scaler = MinMaxScaler((2, 100))\n",
    "whole_trips_df['Scaled Duration'] = scaler.fit_transform(whole_trips_df['Scaled Duration'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f37250-23eb-4153-934b-244268fc2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(data=whole_trips_df['Scaled Duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6fbb7ee-b76b-4c48-9492-3a828ca7a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = whole_trips_df[['Start Time', 'End Time', 'Start Station Id', 'End Station Id', 'Trip Id']].groupby(['Start Station Id', 'End Station Id']).agg({'Trip Id': 'count', 'Start Time': 'min', 'End Time': 'max'}).reset_index().rename({'Trip Id': 'Trip Count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65995ef5-0296-498e-be51-88492775ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['Link Duration'] = temp_df.apply(lambda x: (x['End Time'] - x['Start Time']).ceil('d'), axis=1).astype('timedelta64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bab4a1d-42b3-4d5c-95c1-19085c9a91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['Avg Duration'] = whole_trips_df[['Start Station Id', 'End Station Id', 'Trip Duration']].groupby(['Start Station Id', 'End Station Id']).mean().reset_index()['Trip Duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f77ecd3-b921-4869-a29b-61a7ce256f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['duration weights'] = temp_df['Avg Duration'].apply(lambda x: 1 / x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75e880cc-8156-458d-9eb7-3687ca11220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['tpd weights'] = temp_df.apply(lambda x: x['Trip Count'] / x['Link Duration'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec4e0b3e-03eb-4a04-9ef8-11d77aad7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.rename({'Trip Count': 'trip count weights'}, axis=1).drop(['Start Time', 'End Time', 'Link Duration', 'Avg Duration'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a53624d1-32ed-4931-9106-08fcfacb28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nnp = np.percentile(temp_df[['trip count weights', 'duration weights', 'tpd weights']], 99, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "149282b3-138a-4397-bb2c-590c7747256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df[['trip count weights', 'duration weights', 'tpd weights']] = np.clip(temp_df[['trip count weights', 'duration weights', 'tpd weights']], None, nnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a2237e1-925f-4c7d-803a-94a2e9301b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler((1, 10))\n",
    "x_scaled = min_max_scaler.fit_transform(temp_df[['trip count weights', 'duration weights', 'tpd weights']])\n",
    "temp_df[['trip count weights', 'duration weights', 'tpd weights']] = x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c934c2fd-bb26-4998-b70e-67a97f030a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv('weight_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "103ba7ca-6129-4b18-91bb-af9c1670b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.from_pandas_edgelist(temp_df, source=\"Start Station Id\", target=\"End Station Id\", edge_attr=['trip count weights', 'duration weights', 'tpd weights'], create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0dc731ac-b1a0-473d-aa51-2ba1d5a95c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary directed\n",
    "\n",
    "binary = nx.adjacency_matrix(G, weight=None).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63e8f9d6-5d30-499a-868a-48ee41304249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted (duration) directed\n",
    "\n",
    "duration_weighted = nx.adjacency_matrix(G, weight='duration weights').toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6889dba4-f0b2-421f-9fc7-97ff201ee0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted (trips) directed\n",
    "\n",
    "trips_weighted = nx.adjacency_matrix(G, weight='trips weights').toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ebf52bb8-5253-412d-8e40-11992f40bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "\n",
    "savemat('bst.mat', {'binary': binary, 'trips_w': trips_weighted, 'duration_w': duration_weighted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3144504-682e-4854-accc-d6aeb577342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_edgelist(G, 'bst_full_norm.edgelist', data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
